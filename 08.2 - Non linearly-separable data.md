
# Optimization Problems

## Cases of optimization problems

For future analisys, It is useful to discuss what are the main classes
of optimization problems:

- _linear programming) (LP): linear problem, linear constraints.

$$min_{x} c^Tx\ s.t.\ Ax = b, x \ge 0$$

- _quadratic programming_ (QP): quadratic objective and linear
  constraints, it is convex if the matrix $Q$ is positive
  semidefinite, that is the real number $x^TQx$ is positive or zero for
  every nonzero real column vector $x$, where $x^T$ is the row vector
  transpose of $x$.
  
$$min_{x} c^Tx + \frac{1}{2}x^TQx\ s.t\ Ax = b, Cx \ge d$$

- _nonlinear programming_ (NLP): in general non-convex.

## Solving quadratic problems - Lagrange multipliers

Quadratic optimization problems such as the one discussed above are a well-known class of mathematical programming models with several algorithms. We will now introduce a method so solve such problems using the Lagrange multiplier, that is a strategy for finding the local maxima and minima of a function subject to equation constraints.

Given a function to optimize $f(x)$, a constraint $g(x)$ and an
optimal solution $x_*$ of the function that respects the contraints, there
exists a _lagrangian multiplier_ $\lambda$ such that:
$$\frac{df(x_*)}{dx_*} = \lambda \frac{dg(x_*)}{dx_*},\ g(x) = 0$$
Or equivalently:
 $$\frac{df(x_*)}{dx_*} - \lambda \frac{dg(x_*)}{dx_*} = 0,\ g(x) = 0$$
 We call this the lagrangian function or _Lagrangian_:
 $$L(x) = f(x) - \lambda g(x)$$
 
 Let's now apply this knowledge in our problem. Let $f(x)=||\vec{w}||^2$
 and $g(x, b, w)=y_i(\vec{w}\vec{x_i}+b)-1$, using $a$ as the lagrangian
 multiplier:
 $$(a) L(x, \vec{w}, b, \vec{a}) = \frac{1}{2}||\vec{w}||^2 - \sum_i a_i (y_i(\vec{w}^T\vec{x_i} + b) - 1)$$
 This is an example of Lagrangian dual problem, where we need to
 maximize the Lagrangian multipliers to minimize $w$ and $b$. We now
 derivate with respect to $w$ and $b$ and set them equal to $0$:
 $$(b)\ \vec{w} - \sum_i a_i y_i x_i = 0$$
$$(c)\ \sum_i a_i y_i = 0$$
 
 From $(b)$ we get $\vec{w} = \sum_i a_i y_i x_i$. We now
 substitute the new $(b)$ in $(a)$, observing that $||w||^2 = w^Tw$:
 $$L(x, \vec{a}, b) = \frac{1}{2}\sum_i \sum_j a_i a_j y_i y_j x_i x_j - (\sum_i \sum_j a_i a_j y_i y_j x_i x_j - b\sum_i a_i j_i - \sum_i a_i) $$
 $$ = -\frac{1}{2}\sum_i \sum_i \sum_j a_i a_j y_i y_j x_i x_j - (-b\sum_i a_i y_i - \sum_i a_i) $$
 
 The second term is $0$ because of $(c)$, so It can be eliminated, finally
 we have:
$$ L(x, \vec{a}) = \sum_i a_i -\frac{1}{2}\sum_i \sum_j \sum_j a_i a_j y_i y_j x_i x_j $$
 such that $\sum_i a_i y_i = 0, 0 \le a_i \le C\ \forall i$.
 
 This is the final equation that we need to maximize over $a_i$ to
 minimize w and b. To recap, we turned the original optimization
 problem $min_{w, b} ||\vec{w}||^2$ to a problem depending only on
 lagrangian multipliers, which is faster to compute. We let the
 computer solve this and get the $a_i$ values, after that we can find $w$
 using $(b)$ and b from $y_k = wx_k + b$ for any $k$ and using again $w$
 from $(b)$.
 
 Finally, to make predictions, we use the perceptron formula with $(b)$:
 $$(d)\ f(x) = \sum_i a_iy_i x_i x + b$$
 
 - each non-zero $a_i$ indicates that the corresponding $x_i$ is a support
   vector.
 
## Non linear SVM - Kernel Trick

What if the data is not linearly separable? In such situation we can
map data to a higher-dimensional space where the training set is
separable.
$$\Phi: X \rightarrow H$$
$$h=\phi(x)$$

We notice that the linear classifier (d) relies on the product between
$x_i$ and $x$. We can abstract this product to happen in a higher dimension
using a function called Kernel which computes the dot product over some
higher-dimensional feature mapping function $\phi(x)$:
$$K(x_i, x_j) = \phi(x_i)\cdot \phi(x_j)$$
Therefore $(d)$ becomes:
$$ f(x) = \sum_i a_iy_i K(x_i, x) + b $$

Mercer's Theorem: every positive semidefinite symmetric function is a
kernel

There are multiple types of kernels, such as

- linear: $K(x_i, x_j) = x_i^T x_j$
- polinomial of power p: $K(x_i, x_j) = (1+x_i^T x_j)^p$
- gaussian: $K(x_i, x_j) = e^{\frac{|x_i-x_j|^2}{2\sigma ^2}}$

To recap, kernels are generalization of dot products to arbitrary domains.


